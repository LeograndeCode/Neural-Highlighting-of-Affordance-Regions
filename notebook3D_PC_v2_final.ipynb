{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"BzacayYuedGi"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive') #replace with drive.mount('/content/drive/', force_remount=True) if the drive has changed since last mount in order to force the remount\n","%cd /content/drive/MyDrive/Neural-Highlighting-of-Affordance-Regions/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yMBlhrYSSTyu"},"outputs":[],"source":["!apt-get update\n","!apt-get install -y xvfb ffmpeg libsm6 libxext6\n","!pip install git+https://github.com/openai/CLIP.git\n","!pip install kaolin==0.17.0 -f https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.5.1_cu121.html\n","!pip install open3d pyvirtualdisplay"]},{"cell_type":"markdown","source":["**PointCLIP**: config and dependencies\n"],"metadata":{"id":"J3I9StyNMQMl"}},{"cell_type":"code","source":["%cd PointCLIP\n","!pip install -r requirements.txt\n","%cd Dassl3D/\n","!python setup.py develop\n","%cd ..\n","%cd .."],"metadata":{"id":"x68IF1_9GeQX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here we import from PointCLIP the customized PointCLIP_ZS class, which will be used to perform model inference."],"metadata":{"id":"Dsmg6QkTMmzT"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6A37o1k6Uugk"},"outputs":[],"source":["import sys\n","sys.path.append('/content/drive/MyDrive/Neural-Highlighting-of-Affordance-Regions/PointCLIP/trainers')\n","from zeroshot import PointCLIP_ZS\n","\n","#Available clip models\n","import clip\n","clip.available_models()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zwjtz3i1STyw"},"outputs":[],"source":["import clip\n","import copy\n","import json\n","import kaolin as kal\n","import kaolin.ops.mesh\n","import numpy as np\n","import os\n","import random\n","import torch\n","import torch.nn as nn\n","import torchvision\n","from torchvision import transforms\n","import torchvision.transforms.functional as F\n","\n","from itertools import permutations, product\n","from Normalization import MeshNormalizer\n","from render import Renderer\n","from mesh import Mesh\n","from pathlib import Path\n","from tqdm import tqdm\n","from torch.autograd import grad\n","from torchvision import transforms\n","from utils import device, color_mesh\n","import open3d as o3d\n","from pyvirtualdisplay import Display\n","\n","width = 256\n","depth = 6       #default is 4\n","out_dim = 2\n","input_dim = 3\n","n_augs = 1      #default is 1\n","\n","class NeuralHighlighter(nn.Module):\n","    def __init__(self):\n","        super(NeuralHighlighter, self).__init__()\n","        input_size = 3 #Dimension of the vertex\n","        output_size = 2 #Dimension of the output\n","\n","                        #for the standard highlighter task there are only 2 classes: target region and not target region.\n","                        #we use the element of the output vector corresponding to the probability of belonging to the target\n","                        #region as the highlight probability described in the main paper.\n","        layers = []\n","\n","        #first linear layer followed by ReLU and LayerNorm\n","        layers.append(nn.Linear(input_dim, width))\n","        layers.append(nn.ReLU())\n","        layers.append(nn.LayerNorm([width]))\n","        #other [depth] linear layers followed by ReLU and LayerNorm\n","        # -> changing the depth hyperparameter results in a deeper/shallower net\n","        # -> total depth (in terms of modules[Linear+ReLU+LayerNorm]) = [depth] + 1\n","        for i in range(depth):\n","            layers.append(nn.Linear(width, width))\n","            layers.append(nn.ReLU())\n","            layers.append(nn.LayerNorm([width]))\n","        #last linear layer followed by softmax in order to output probability-like values\n","        layers.append(nn.Linear(width, out_dim))\n","        layers.append(nn.Softmax(dim=1))\n","\n","        self.mlp = nn.ModuleList(layers)\n","        self.model = self.mlp\n","        print(self.mlp)\n","\n","    def forward(self, x):\n","        for layer in self.model:\n","            x = layer(x)\n","        return x\n","\n","def get_clip_model(clipmodel):\n","    model, preprocess = clip.load(clipmodel, device=device)\n","    return model, preprocess\n","\n","# ================== HELPER FUNCTIONS =============================\n","def save_final_results(log_dir, name, mesh, mlp, vertices, colors, render, background):\n","    mlp.eval()\n","    with torch.no_grad():\n","        probs = mlp(vertices)\n","        max_idx = torch.argmax(probs, 1, keepdim=True)\n","        # for renders\n","        one_hot = torch.zeros(probs.shape).to(device)\n","        one_hot = one_hot.scatter_(1, max_idx, 1)\n","        sampled_mesh = mesh\n","\n","        highlight = torch.tensor([204, 255, 0]).to(device)\n","        gray = torch.tensor([180, 180, 180]).to(device)\n","        colors = torch.stack((highlight/255, gray/255)).to(device)\n","        color_mesh(one_hot, sampled_mesh, colors)\n","        rendered_images, _, _ = render.render_views(sampled_mesh, num_views=5,\n","                                                                        show=False,\n","                                                                        center_azim=0,\n","                                                                        center_elev=0,\n","                                                                        std=1,\n","                                                                        return_views=True,\n","                                                                        lighting=True,\n","                                                                        background=background)\n","        # for mesh\n","        final_color = torch.zeros(vertices.shape[0], 3).to(device)\n","        final_color = torch.where(max_idx==0, highlight, gray)\n","        mesh.export(os.path.join(log_dir, f\"{name}.ply\"), extension=\"ply\", color=final_color)\n","        save_renders(log_dir, 0, rendered_images, name='final_render.jpg')\n","\n","def save_point_cloud_results(vertices, log_dir, name, mlp):\n","\n","    mlp.eval()\n","    with torch.no_grad():\n","      probs = mlp(vertices)\n","\n","      highlight = torch.tensor([204, 255, 0]).to(device)\n","      gray = torch.tensor([180, 180, 180]).to(device)\n","      colors = torch.stack((highlight/255, gray/255)).to(device)\n","\n","      point_colors = assign_colors(probs, colors, device)\n","      point_cloud = o3d.geometry.PointCloud()\n","      point_cloud.points = o3d.utility.Vector3dVector(vertices)\n","      point_cloud.colors = o3d.utility.Vector3dVector(colors)\n","\n","      o3d.io.write_point_cloud(f\"{name}_colored_point_cloud.ply\", point_cloud)\n","\n","def clip_loss(rendered_images, encoded_text, clip_transform, augment_transform, clip_model):\n","    if n_augs == 0:\n","        clip_image = clip_transform(rendered_images)\n","        encoded_renders = clip_model.encode_image(clip_image)\n","        encoded_renders = encoded_renders / encoded_renders.norm(dim=1, keepdim=True)\n","        if encoded_text.shape[0] > 1:\n","            loss = torch.cosine_similarity(torch.mean(encoded_renders, dim=0),\n","                                                torch.mean(encoded_text, dim=0), dim=0)\n","        else:\n","            loss = torch.cosine_similarity(torch.mean(encoded_renders, dim=0, keepdim=True),\n","                                                encoded_text)\n","\n","    elif n_augs > 0:\n","        loss = 0.0\n","        for _ in range(n_augs):\n","            augmented_image = augment_transform(rendered_images)\n","            encoded_renders = clip_model.encode_image(augmented_image)\n","            if encoded_text.shape[0] > 1:\n","                loss -= torch.cosine_similarity(torch.mean(encoded_renders, dim=0),\n","                                                    torch.mean(encoded_text, dim=0), dim=0)\n","            else:\n","                loss -= torch.cosine_similarity(torch.mean(encoded_renders, dim=0, keepdim=True),\n","                                                    encoded_text)\n","    return loss\n","\n","def pointClip_loss(point_cloud, point_cloud_colors, pointClipZS):\n","\n","    pc_to_pass = torch.cat([point_cloud, point_cloud_colors], dim=1)\n","    print(\"pc_to_pass_shape: \", pc_to_pass.shape)\n","\n","    logits = pointClipZS.model_inference(pc_to_pass)\n","    loss = torch.sigmoid(torch.tensor(logits, device=device, requires_grad = True))\n","\n","    return loss\n","\n","def save_renders(dir, i, rendered_images, name=None):\n","    if name is not None:\n","        torchvision.utils.save_image(rendered_images, os.path.join(dir, name))\n","    else:\n","        torchvision.utils.save_image(rendered_images, os.path.join(dir, 'renders/iter_{}.jpg'.format(i)))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NW_UP8wESTyx"},"outputs":[],"source":["from torch.optim.lr_scheduler import MultiStepLR\n","from utils import assign_colors\n","\n","# Constrain most sources of randomness\n","# (some torch backwards functions within CLIP are non-determinstic)\n","seed = 0\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True\n","\n","render_res = 224\n","learning_rate = 0.01\n","n_iter = 2500\n","res = 224\n","obj_path = 'data/dog.obj'\n","#output_dir = './output/'\n","clip_model_name = 'ViT-B/16'\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","#Path(os.path.join(output_dir, 'renders')).mkdir(parents=True, exist_ok=True)\n","\n","objbase, extension = os.path.splitext(os.path.basename(obj_path))\n","\n","render = Renderer(dim=(render_res, render_res))\n","\n","#---------- WARNING---------\n","#---------- now i read the mesh from the obj of the reconstructed mesh\n","#mesh = Mesh(obj_path)\n","#MeshNormalizer(mesh)()\n","#---------- WARNING---------\n","\n","width_render, height_render = 1400, 900\n","zoom_out_factor = 0.4\n","\n","#------------ MESH TO POINT CLOUD INIT---------------\n","from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(width_render, height_render))\n","display.start()\n","\n","#------- here we retrieve first the point cloud from the mesh (only for test purpose)\n","mesh_o3d = o3d.io.read_triangle_mesh(obj_path)\n","mesh_o3d.compute_vertex_normals()\n","pcd = mesh_o3d.sample_points_poisson_disk(2048)\n","pcd_points = np.asarray(pcd.points)\n","print(\"pcd_points shape\")\n","print(pcd_points.shape)\n","#-------\n","\n","#------- here we retrieve the point cloud from the ply file\n","#pcd = o3d.io.read_point_cloud('./pointClouds/validation/bottle/Bottle_point_cloud_0.ply')\n","#-------\n","render_pc = o3d.visualization.rendering.OffscreenRenderer(width_render, height_render)\n","material = o3d.visualization.rendering.MaterialRecord()\n","material.shader = \"defaultUnlit\"\n","render_pc.scene.add_geometry(\"point_cloud\", pcd, material)\n","\n","bounding_box = pcd.get_axis_aligned_bounding_box()\n","center = bounding_box.get_center()\n","extent = bounding_box.get_extent()\n","render_pc.scene.camera.look_at(center, center + [0, 0, 1], [0, 1, 0])\n","render_pc.scene.camera.set_projection(60 / zoom_out_factor, width_render / height_render, 0.1, 100.0,\n","                                      o3d.visualization.rendering.Camera.FovType.Horizontal)\n","\n","img = render_pc.render_to_image()\n","output_file = \"point_cloud_render.jpg\"\n","o3d.io.write_image(output_file, img)\n","#------------ MESH TO POINT CLOUD END---------------\n","\n","#then we approximate a mesh so we can still use the previously defined helper functions\n","\n","#------------ POINT CLOUD TO MESH INIT---------------\n","\n","radii = [0.005, 0.01, 0.02, 0.04]\n","rec_mesh = o3d.geometry.TriangleMesh.create_from_point_cloud_ball_pivoting(pcd, o3d.utility.DoubleVector(radii))\n","\n","# Optionally, check if the reconstructed mesh is valid\n","if rec_mesh.is_empty():\n","    print(\"Mesh reconstruction failed.\")\n","else:\n","    print(\"Mesh reconstruction successful!\")\n","\n","# Render the reconstructed mesh\n","render_mesh = o3d.visualization.rendering.OffscreenRenderer(width_render, height_render)\n","material_mesh = o3d.visualization.rendering.MaterialRecord()\n","material_mesh.shader = \"defaultUnlit\"\n","render_mesh.scene.add_geometry(\"reconstructed_mesh\", rec_mesh, material_mesh)\n","\n","# Set zoom-out factor and camera for reconstructed mesh\n","bounding_box = pcd.get_axis_aligned_bounding_box()\n","center = bounding_box.get_center()\n","render_mesh.scene.camera.look_at(center, center + [0, 0, 1], [0, 1, 0])\n","render_mesh.scene.camera.set_projection(60 / zoom_out_factor, width_render / height_render, 0.1, 100.0,\n","                                        o3d.visualization.rendering.Camera.FovType.Horizontal)\n","\n","# Render and save the reconstructed mesh image\n","img_mesh = render_mesh.render_to_image()\n","output_file_mesh = \"reconstructed_mesh_render.jpg\"\n","o3d.io.write_image(output_file_mesh, img_mesh)\n","\n","#Export the reconstructed mesh to an obj file (allows to reuse helper functions)\n","output_mesh_file = \"reconstructed_mesh.obj\"\n","o3d.io.write_triangle_mesh(output_mesh_file, rec_mesh)\n","\n","#Import the mesh from the exported obj file\n","mesh = Mesh(output_mesh_file)\n","MeshNormalizer(mesh)()\n","\n","display.stop()\n","#------------ POINT CLOUD TO MESH END---------------\n","\n","# Initialize variables\n","background = torch.tensor((1., 1., 1.)).to(device)\n","\n","#log_dir = output_dir\n","\n","# CLIP and Augmentation Transforms\n","clip_normalizer = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n","\n","clip_transform = transforms.Compose([\n","        transforms.Resize((res, res)),\n","        clip_normalizer\n","])\n","\n","augment_transform = transforms.Compose([\n","        transforms.RandomResizedCrop(res, scale=(1, 1)),\n","        transforms.RandomPerspective(fill=1, p=0.8, distortion_scale=0.5),\n","        clip_normalizer\n","])\n","\n","# MLP Settings\n","mlp = NeuralHighlighter().to(device)\n","optim = torch.optim.Adam(mlp.parameters(), learning_rate)\n","\n","#introducing learning rate decay\n","#with the prompt horse/saddle the loss plateaus\n","#scheduler = StepLR(optim, step_size=300, gamma=0.1)\n","\n","#scheduler = MultiStepLR(optim, milestones=[300, 1800], gamma=0.1)  # Decay at epoch 300 and 1800\n","\n","\n","# list of possible colors\n","rgb_to_color = {(204/255, 1., 0.): \"highlighter\", (180/255, 180/255, 180/255): \"gray\"}\n","color_to_rgb = {\"highlighter\": [204/255, 1., 0.], \"gray\": [180/255, 180/255, 180/255]}\n","full_colors = [[204/255, 1., 0.], [180/255, 180/255, 180/255]]\n","colors = torch.tensor(full_colors).to(device)\n","\n","name = 'dogPointCLIP_d_{}_augs_{}'.format(depth, n_augs)\n","depth_maps_views = 6\n","\n","# --- Prompt ---\n","# encode prompt with CLIP\n","clip_model, preprocess = get_clip_model(clip_model_name)\n","\n","#WARNING: with PointCLIP the prompt is hardcoded in PointCLIP zeroshot file\n","prompts = ['A 3D picture of a gray and white dog with highlighted shoes']\n","\n","\n","for i, prompt in enumerate(prompts):\n","\n","  pointClipZS = PointCLIP_ZS()\n","  output_dir = './output_{}_{}/'.format(name, i)\n","  Path(os.path.join(output_dir, 'renders')).mkdir(parents=True, exist_ok=True)\n","  log_dir = output_dir\n","\n","  #here we compute the text encoding only once\n","  #if we put it inside the loss, we repeat n_iter times the same computation\n","  with torch.no_grad():\n","    text_input = clip.tokenize([prompt]).to(device)\n","    print(text_input.shape)\n","    encoded_text = clip_model.encode_text(text_input)\n","    encoded_text = encoded_text / encoded_text.norm(dim=1, keepdim=True)\n","\n","  vertices = copy.deepcopy(mesh.vertices)\n","  #vertices = torch.tensor(np.asarray(rec_mesh.vertices), dtype=torch.float32, device=device) # Convert vertices to a PyTorch tensor\n","  point_cloud_points = torch.tensor(pcd_points, dtype=torch.float32, device=device) # Convert point cloud points to a PyTorch tensor\n","  n_views = 5\n","\n","  losses = []\n","\n","  # Optimization loop\n","  for i in tqdm(range(n_iter)):\n","    optim.zero_grad()\n","\n","    # predict highlight probabilities\n","    pred_class = mlp(vertices)\n","\n","    # color and render mesh\n","    sampled_mesh = mesh\n","    color_mesh(pred_class, sampled_mesh, colors)\n","\n","    #exporting the colored mesh\n","    max_idx = torch.argmax(pred_class, 1, keepdim=True)\n","    highlight = torch.tensor([204, 255, 0]).to(device)\n","    gray = torch.tensor([180, 180, 180]).to(device)\n","    final_color = torch.zeros(vertices.shape[0], 3).to(device)\n","    final_color = torch.where(max_idx==0, highlight, gray)\n","    mesh.export(os.path.join(log_dir, f\"iter{i}.ply\"), extension=\"ply\", color=final_color)\n","\n","    #reading the open3D mesh from the exported file and deriving the colored point cloud\n","    mesh_o3d = o3d.io.read_triangle_mesh(os.path.join(log_dir, f\"iter{i}.ply\"))\n","    mesh_o3d.compute_vertex_normals()\n","    pcd_train = mesh_o3d.sample_points_poisson_disk(2000)\n","\n","    #extracting points and colors from the derived point cloud\n","    points_train = torch.tensor(np.asarray(pcd_train.points), dtype=torch.float32).cuda()  # Shape: [N, 3]\n","    colors_train = torch.tensor(np.asarray(pcd_train.colors), dtype=torch.float32).cuda()  # Shape: [N, 3]\n","\n","    #Calculate PointCLIP Loss\n","    loss = pointClip_loss(points_train, colors_train, pointClipZS)\n","\n","    loss.backward(retain_graph=True)\n","\n","    optim.step()\n","\n","    #LR decay\n","    #scheduler.step()\n","\n","    # update variables + record loss\n","    with torch.no_grad():\n","        losses.append(loss.item())\n","\n","    # report results\n","    if i % 100 == 0:\n","        print(\"Last 100 CLIP score: {}\".format(np.mean(losses[-100:])))\n","        #save_renders(log_dir, i, rendered_images)\n","        #save_renders(log_dir, i, color_tensor_final)\n","\n","        with open(os.path.join(log_dir, \"training_info.txt\"), \"a\") as f:\n","            f.write(f\"For iteration {i}... Prompt: {prompt}, Last 100 avg CLIP score: {np.mean(losses[-100:])}, CLIP score {losses[-1]}\\n\")\n","\n","  # save point cloud results\n","  save_point_cloud_results(vertices, log_dir, name, mlp)\n","\n","\n","\n","  # Save prompts\n","  with open(os.path.join(output_dir, 'prompt.txt'), \"w\") as f:\n","    f.write(prompt)\n","    f.write(\"\\n\")\n","    f.write(\"initial learning rate:\")\n","    f.write(str(learning_rate))\n","    f.write(\"\\n\")\n","    f.write(\"n_iter:\")\n","    f.write(str(n_iter))\n","    f.write(\"\\n\")\n","    f.write(\"n_augs:\")\n","    f.write(str(n_augs))\n","    f.write(\"\\n\")\n","    f.write(\"n_views:\")\n","    f.write(str(n_views))\n","    f.write(\"\\n\")\n","    f.write(\"clip_model:\")\n","    f.write(clip_model_name)\n","    f.write(\"\\n\")\n","    f.write(\"depth:\")\n","    f.write(str(depth))\n"]},{"cell_type":"code","source":["torch.cuda.empty_cache()"],"metadata":{"id":"AUiPPT3NVQuw"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}