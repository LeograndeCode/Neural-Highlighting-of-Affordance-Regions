{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMAzJ2sgg7Q8tyHoydvcqy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeograndeCode/Neural-Highlighting-of-Affordance-Regions/blob/Parte-3/Notebook3D_AffordanceNet_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwtBWLCynwPb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') #replace with drive.mount('/content/drive/', force_remount=True) if the drive has changed since last mount in order to force the remount\n",
        "%cd /content/drive/MyDrive/Neural-Highlighting-of-Affordance-Regions/\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y xvfb ffmpeg libsm6 libxext6\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install kaolin==0.17.0 -f https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.5.1_cu121.html\n",
        "!pip install open3d pyvirtualdisplay"
      ],
      "metadata": {
        "id": "2_ga4ZXWtU-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###AffordanceNet Class"
      ],
      "metadata": {
        "id": "vFNWfwgnthjl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AffordanceNet Class modified with capability of choosing only some objects, in our case household objects with hand-object affordances\n",
        "\n"
      ],
      "metadata": {
        "id": "SQpppbGPoYW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from os.path import join as opj\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "import h5py\n",
        "import json\n",
        "from utils.provider import rotate_point_cloud_SO3, rotate_point_cloud_y\n",
        "import pickle as pkl\n",
        "\n",
        "\n",
        "# List of objects you're interested in\n",
        "valid_objects = [\"Bowl\", \"Bottle\", \"Door\", \"Faucet\", \"Knife\", \"Mug\", \"Scissors\", \"Vase\"]\n",
        "\n",
        "# List of affordances you're interested in\n",
        "valid_affordances = [\"grasp\", \"push\", \"wrap\", \"pour\", \"wrap-grasp\", \"open\", \"pull\"]\n",
        "\n",
        "def pc_normalize(pc):\n",
        "    centroid = np.mean(pc, axis=0)\n",
        "    pc = pc - centroid\n",
        "    m = np.max(np.sqrt(np.sum(pc**2, axis=1)))\n",
        "    pc = pc / m\n",
        "    return pc, centroid, m\n",
        "\n",
        "\n",
        "def semi_points_transform(points):\n",
        "    spatialExtent = np.max(points, axis=0) - np.min(points, axis=0)\n",
        "    eps = 2e-3*spatialExtent[np.newaxis, :]\n",
        "    jitter = eps*np.random.randn(points.shape[0], points.shape[1])\n",
        "    points_ = points + jitter\n",
        "    return points_\n",
        "\n",
        "\n",
        "class AffordNetDataset(Dataset):\n",
        "    def __init__(self, data_dir, split, partial=False, rotate='None', semi=False):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.split = split\n",
        "\n",
        "        self.partial = partial\n",
        "        self.rotate = rotate\n",
        "        self.semi = semi\n",
        "\n",
        "        self.load_data()\n",
        "\n",
        "        self.affordance = self.all_data[0][\"affordance\"]\n",
        "\n",
        "        return\n",
        "\n",
        "    def load_data(self):\n",
        "        self.all_data = []\n",
        "\n",
        "        # Open the dataset file (depending on partial or not)\n",
        "        if self.semi:\n",
        "            with open(opj(self.data_dir, 'semi_label_1.pkl'), 'rb') as f:\n",
        "                temp_data = pkl.load(f)\n",
        "        else:\n",
        "            if self.partial:\n",
        "                with open(opj(self.data_dir, 'partial_%s_data.pkl' % self.split), 'rb') as f:\n",
        "                    temp_data = pkl.load(f)\n",
        "            elif self.rotate != \"None\" and self.split != 'train':\n",
        "                with open(opj(self.data_dir, 'rotate_%s_data.pkl' % self.split), 'rb') as f:\n",
        "                    temp_data_rotate = pkl.load(f)\n",
        "                with open(opj(self.data_dir, 'full_shape_%s_data.pkl' % self.split), 'rb') as f:\n",
        "                    temp_data = pkl.load(f)\n",
        "            else:\n",
        "                with open(opj(self.data_dir, 'full_shape_%s_data.pkl' % self.split), 'rb') as f:\n",
        "                    temp_data = pkl.load(f)\n",
        "\n",
        "        for index, info in enumerate(temp_data):\n",
        "            # Only load data for valid objects\n",
        "            if info[\"semantic class\"] not in valid_objects:\n",
        "                continue\n",
        "\n",
        "            if self.partial:\n",
        "                partial_info = info[\"partial\"]\n",
        "                for view, data_info in partial_info.items():\n",
        "                    temp_info = {}\n",
        "                    temp_info[\"shape_id\"] = info[\"shape_id\"]\n",
        "                    temp_info[\"semantic class\"] = info[\"semantic class\"]\n",
        "                    temp_info[\"affordance\"] = info[\"affordance\"]\n",
        "                    temp_info[\"view_id\"] = view\n",
        "                    temp_info[\"data_info\"] = data_info\n",
        "\n",
        "                    # Filter out unwanted affordances\n",
        "                    temp_info[\"affordance\"] = {key: value for key, value in temp_info[\"affordance\"].items() if key in valid_affordances}\n",
        "\n",
        "                    self.all_data.append(temp_info)\n",
        "            elif self.split != 'train' and self.rotate != 'None':\n",
        "                rotate_info = temp_data_rotate[index][\"rotate\"][self.rotate]\n",
        "                full_shape_info = info[\"full_shape\"]\n",
        "                for r, r_data in rotate_info.items():\n",
        "                    temp_info = {}\n",
        "                    temp_info[\"shape_id\"] = info[\"shape_id\"]\n",
        "                    temp_info[\"semantic class\"] = info[\"semantic class\"]\n",
        "                    temp_info[\"affordance\"] = info[\"affordance\"]\n",
        "                    temp_info[\"data_info\"] = full_shape_info\n",
        "                    temp_info[\"rotate_matrix\"] = r_data.astype(np.float32)\n",
        "\n",
        "                    # Filter out unwanted affordances\n",
        "                    temp_info[\"affordance\"] = {key: value for key, value in temp_info[\"affordance\"].items() if key in valid_affordances}\n",
        "\n",
        "                    self.all_data.append(temp_info)\n",
        "            else:\n",
        "                temp_info = {}\n",
        "                temp_info[\"shape_id\"] = info[\"shape_id\"]\n",
        "                temp_info[\"semantic class\"] = info[\"semantic class\"]\n",
        "                temp_info[\"affordance\"] = info[\"affordance\"]\n",
        "                temp_info[\"data_info\"] = info[\"full_shape\"]\n",
        "\n",
        "                # Filter out unwanted affordances\n",
        "                temp_info[\"affordance\"] = {key: value for key, value in temp_info[\"affordance\"].items() if key in valid_affordances}\n",
        "\n",
        "                self.all_data.append(temp_info)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data_dict = self.all_data[index]\n",
        "        modelid = data_dict[\"shape_id\"]\n",
        "        modelcat = data_dict[\"semantic class\"]\n",
        "\n",
        "        data_info = data_dict[\"data_info\"]\n",
        "        model_data = data_info[\"coordinate\"].astype(np.float32)\n",
        "        labels = data_info[\"label\"]\n",
        "        for aff in self.affordance:\n",
        "            temp = labels[aff].astype(np.float32).reshape(-1, 1)\n",
        "            model_data = np.concatenate((model_data, temp), axis=1)\n",
        "\n",
        "        datas = model_data[:, :3]\n",
        "        targets = model_data[:, 3:]\n",
        "\n",
        "        if self.rotate != 'None':\n",
        "            if self.split == 'train':\n",
        "                if self.rotate == 'so3':\n",
        "                    datas = rotate_point_cloud_SO3(\n",
        "                        datas[np.newaxis, :, :]).squeeze()\n",
        "                elif self.rotate == 'z':\n",
        "                    datas = rotate_point_cloud_y(\n",
        "                        datas[np.newaxis, :, :]).squeeze()\n",
        "            else:\n",
        "                r_matrix = data_dict[\"rotate_matrix\"]\n",
        "                datas = (np.matmul(r_matrix, datas.T)).T\n",
        "\n",
        "        datas, _, _ = pc_normalize(datas)\n",
        "\n",
        "        return datas, datas, targets, modelid, modelcat\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_data)\n",
        "\n",
        "\n",
        "class AffordNetDataset_Unlabel(Dataset):\n",
        "    def __init__(self, data_dir):\n",
        "        super().__init__()\n",
        "        self.data_dir = data_dir\n",
        "        self.load_data()\n",
        "        self.affordance = self.all_data[0][\"affordance\"]\n",
        "        return\n",
        "\n",
        "    def load_data(self):\n",
        "        self.all_data = []\n",
        "        with open(opj(self.data_dir, 'semi_unlabel_1.pkl'), 'rb') as f:\n",
        "            temp_data = pkl.load(f)\n",
        "\n",
        "        for info in temp_data:\n",
        "            # Only load data for valid objects\n",
        "            if info[\"semantic class\"] not in valid_objects:\n",
        "                continue\n",
        "\n",
        "            temp_info = {}\n",
        "            temp_info[\"shape_id\"] = info[\"shape_id\"]\n",
        "            temp_info[\"semantic class\"] = info[\"semantic class\"]\n",
        "            temp_info[\"affordance\"] = info[\"affordance\"]\n",
        "            temp_info[\"data_info\"] = info[\"full_shape\"]\n",
        "\n",
        "            # Filter out unwanted affordances\n",
        "            temp_info[\"affordance\"] = {key: value for key, value in temp_info[\"affordance\"].items() if key in valid_affordances}\n",
        "\n",
        "            self.all_data.append(temp_info)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data_dict = self.all_data[index]\n",
        "        modelid = data_dict[\"shape_id\"]\n",
        "        modelcat = data_dict[\"semantic class\"]\n",
        "\n",
        "        data_info = data_dict[\"data_info\"]\n",
        "        datas = data_info[\"coordinate\"].astype(np.float32)\n",
        "\n",
        "        datas, _, _ = pc_normalize(datas)\n",
        "\n",
        "        return datas, datas, modelid, modelcat\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_data)\n"
      ],
      "metadata": {
        "id": "GKHLTK2MqnOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "2WDvKTKYtOT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "import copy\n",
        "import json\n",
        "import kaolin as kal\n",
        "import kaolin.ops.mesh\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "from itertools import permutations, product\n",
        "from Normalization import MeshNormalizer\n",
        "from render import Renderer\n",
        "from mesh import Mesh\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from torch.autograd import grad\n",
        "from torchvision import transforms\n",
        "from utils import device, color_mesh\n",
        "import open3d as o3d\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "width = 256\n",
        "depth = 4       #default is 4\n",
        "out_dim = 2\n",
        "input_dim = 3\n",
        "n_augs = 1      #default is 1\n",
        "\n",
        "class NeuralHighlighter(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralHighlighter, self).__init__()\n",
        "        input_size = 3 #Dimension of the vertex\n",
        "        output_size = 2 #Dimension of the output\n",
        "                        #for the standard highlighter task there are only 2 classes: target region and not target region.\n",
        "                        #we use the element of the output vector corresponding to the probability of belonging to the target\n",
        "                        #region as the highlight probability described in the main paper.\n",
        "        layers = []\n",
        "\n",
        "        #See Appendix B (page 13)\n",
        "        #first linear layer followed by ReLU and LayerNorm\n",
        "        layers.append(nn.Linear(input_dim, width))\n",
        "        layers.append(nn.ReLU())\n",
        "        layers.append(nn.LayerNorm([width]))\n",
        "        #other [depth] linear layers followed by ReLU and LayerNorm\n",
        "        # -> changing the depth hyperparameter results in a deeper/shallower net\n",
        "        # -> total depth (in terms of modules[Linear+ReLU+LayerNorm]) = [depth] + 1\n",
        "        for i in range(depth):\n",
        "            layers.append(nn.Linear(width, width))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.LayerNorm([width]))\n",
        "        #last linear layer followed by softmax in order to output probability-like values\n",
        "        layers.append(nn.Linear(width, out_dim))\n",
        "        layers.append(nn.Softmax(dim=1))\n",
        "\n",
        "        self.mlp = nn.ModuleList(layers)\n",
        "        self.model = self.mlp\n",
        "        print(self.mlp)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.model:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "def get_clip_model(clipmodel):\n",
        "    model, preprocess = clip.load(clipmodel, device=device)\n",
        "    return model, preprocess\n",
        "\n",
        "# ================== HELPER FUNCTIONS =============================\n",
        "def save_final_results(log_dir, name, mesh, mlp, vertices, colors, render, background):\n",
        "    mlp.eval()\n",
        "    with torch.no_grad():\n",
        "        probs = mlp(vertices)\n",
        "        max_idx = torch.argmax(probs, 1, keepdim=True)\n",
        "        # for renders\n",
        "        one_hot = torch.zeros(probs.shape).to(device)\n",
        "        one_hot = one_hot.scatter_(1, max_idx, 1)\n",
        "        sampled_mesh = mesh\n",
        "\n",
        "        highlight = torch.tensor([204, 255, 0]).to(device)\n",
        "        gray = torch.tensor([180, 180, 180]).to(device)\n",
        "        colors = torch.stack((highlight/255, gray/255)).to(device)\n",
        "        color_mesh(one_hot, sampled_mesh, colors)\n",
        "        rendered_images, _, _ = render.render_views(sampled_mesh, num_views=5,\n",
        "                                                                        show=False,\n",
        "                                                                        center_azim=0,\n",
        "                                                                        center_elev=0,\n",
        "                                                                        std=1,\n",
        "                                                                        return_views=True,\n",
        "                                                                        lighting=True,\n",
        "                                                                        background=background)\n",
        "        # for mesh\n",
        "        final_color = torch.zeros(vertices.shape[0], 3).to(device)\n",
        "        final_color = torch.where(max_idx==0, highlight, gray)\n",
        "        mesh.export(os.path.join(log_dir, f\"{name}.ply\"), extension=\"ply\", color=final_color)\n",
        "        save_renders(log_dir, 0, rendered_images, name='final_render.jpg')\n",
        "\n",
        "#TODO: fix the generation of the point cloud subsequently\n",
        "#      now the point cloud generation is possible only by executing the PC_rendering.ipynb\n",
        "\n",
        "def save_point_cloud_results(log_dir, name):\n",
        "        #now i load the highlighted mesh and transpose it back to the point cloud\n",
        "        display = Display(visible=0, size=(1400, 900))\n",
        "        display.start()\n",
        "        mesh_o3d = o3d.io.read_triangle_mesh(os.path.join(log_dir, f\"{name}.ply\"))\n",
        "\n",
        "        if not mesh_o3d.has_vertex_normals():\n",
        "          mesh_o3d.compute_vertex_normals()\n",
        "\n",
        "        point_cloud = mesh_o3d.sample_points_poisson_disk(number_of_points=10000)\n",
        "\n",
        "        width_final_render, height_final_render = 1400, 900\n",
        "        render_final_pc = o3d.visualization.rendering.OffscreenRenderer(width_final_render, height_final_render)\n",
        "        material = o3d.visualization.rendering.MaterialRecord()\n",
        "        material.shader = \"defaultUnlit\"\n",
        "        render_final_pc.scene.add_geometry(\"point_cloud\", point_cloud, material)\n",
        "\n",
        "        zoom_out_factor = 0.5\n",
        "        bounding_box = point_cloud.get_axis_aligned_bounding_box()\n",
        "        center = bounding_box.get_center()\n",
        "        extent = bounding_box.get_extent()\n",
        "        render_final_pc.scene.camera.look_at(center, center + [0, 0, 1], [0, 1, 0])\n",
        "        render_final_pc.scene.camera.set_projection(60 / zoom_out_factor, width_final_render / height_final_render, 0.1, 100.0,\n",
        "                                      o3d.visualization.rendering.Camera.FovType.Horizontal)\n",
        "\n",
        "        pc_img = render_final_pc.render_to_image()\n",
        "        output_file = os.path.join(log_dir, f\"{name}_final_render.jpg\")\n",
        "        o3d.io.write_image(output_file, pc_img)\n",
        "        display.stop()\n",
        "\n",
        "\n",
        "def clip_loss(rendered_images, encoded_text, clip_transform, augment_transform, clip_model):\n",
        "    if n_augs == 0:\n",
        "        clip_image = clip_transform(rendered_images)\n",
        "        encoded_renders = clip_model.encode_image(clip_image)\n",
        "        encoded_renders = encoded_renders / encoded_renders.norm(dim=1, keepdim=True)\n",
        "        if encoded_text.shape[0] > 1:\n",
        "            loss = torch.cosine_similarity(torch.mean(encoded_renders, dim=0),\n",
        "                                                torch.mean(encoded_text, dim=0), dim=0)\n",
        "        else:\n",
        "            loss = torch.cosine_similarity(torch.mean(encoded_renders, dim=0, keepdim=True),\n",
        "                                                encoded_text)\n",
        "\n",
        "    elif n_augs > 0:\n",
        "        loss = 0.0 #original 0.0\n",
        "        for _ in range(n_augs):\n",
        "            augmented_image = augment_transform(rendered_images)\n",
        "            encoded_renders = clip_model.encode_image(augmented_image)\n",
        "            if encoded_text.shape[0] > 1:\n",
        "                loss -= torch.cosine_similarity(torch.mean(encoded_renders, dim=0),\n",
        "                                                    torch.mean(encoded_text, dim=0), dim=0)\n",
        "            else:\n",
        "                loss -= torch.cosine_similarity(torch.mean(encoded_renders, dim=0, keepdim=True),\n",
        "                                                    encoded_text)\n",
        "    return loss\n",
        "\n",
        "def save_renders(dir, i, rendered_images, name=None):\n",
        "    if name is not None:\n",
        "        torchvision.utils.save_image(rendered_images, os.path.join(dir, name))\n",
        "    else:\n",
        "        torchvision.utils.save_image(rendered_images, os.path.join(dir, 'renders/iter_{}.jpg'.format(i)))"
      ],
      "metadata": {
        "id": "yXjytggktZyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TO-DO Make sure it's correct\n"
      ],
      "metadata": {
        "id": "R1W-wF0O1yMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "for torch.utils.data import Subset, DataLoader\n",
        "\n",
        "def split_dataset(dataset, train_size=0.7, val_size=0.1, test_size=0.2):\n",
        "    # First split: 80% for training and 20% for temp (which will be further split into validation and test)\n",
        "    train_data, temp_data = train_test_split(dataset, test_size=(val_size + test_size), random_state=42)\n",
        "\n",
        "    # Second split: divide the 20% temp_data into 50% validation and 50% test\n",
        "    val_size_adjusted = val_size / (val_size + test_size)\n",
        "    val_data, test_data = train_test_split(temp_data, test_size=(1 - val_size_adjusted), random_state=42)\n",
        "\n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "# Assuming AffordNetDataset class is initialized correctly and accepts a `split` argument\n",
        "data_dir = \"./dataset\"\n",
        "dataset = AffordNetDataset(data_dir=data_dir, split=\"train\", partial=False, rotate=\"None\", semi=False)\n",
        "\n",
        "# Split the dataset into train (70%), validation (10%), and test (20%)\n",
        "train_dataset, val_dataset, test_dataset = split_dataset(dataset, train_size=0.7, val_size=0.1, test_size=0.2)\n",
        "\n",
        "# Now define the DataLoaders for each split\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers = 4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers = 4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers = 4)\n",
        "\n",
        "# Check dataset sizes\n",
        "print('Train Dataset: {}'.format(len(train_dataset)))\n",
        "print('Valid Dataset: {}'.format(len(val_dataset)))\n",
        "print('Test Dataset: {}'.format(len(test_dataset)))\n"
      ],
      "metadata": {
        "id": "_SnyUD5n0s_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " TO-DO Upsample Pointclouds to match the 10000 points of the 3d Highlighter and make sure they are both normalized"
      ],
      "metadata": {
        "id": "HnWJ_F_d1mtF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRAIN\n"
      ],
      "metadata": {
        "id": "69ewBl90t7at"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TO-DO Modify Training"
      ],
      "metadata": {
        "id": "s8_0K46717Tt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "# Constrain most sources of randomness\n",
        "# (some torch backwards functions within CLIP are non-determinstic)\n",
        "seed = 0\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "render_res = 224\n",
        "learning_rate = 0.0001\n",
        "n_iter = 2500\n",
        "res = 224\n",
        "obj_path = 'data/dog.obj'\n",
        "#output_dir = './output/'\n",
        "clip_model_name = 'ViT-B/32'\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "#Path(os.path.join(output_dir, 'renders')).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "objbase, extension = os.path.splitext(os.path.basename(obj_path))\n",
        "\n",
        "render = Renderer(dim=(render_res, render_res))\n",
        "\n",
        "#then we approximate a mesh so we can still use the previously defined helper functions\n",
        "#also the loss minimization should converge better\n",
        "\n",
        "\n",
        "# Initialize variables\n",
        "background = torch.tensor((1., 1., 1.)).to(device)\n",
        "\n",
        "#log_dir = output_dir\n",
        "\n",
        "# CLIP and Augmentation Transforms\n",
        "clip_normalizer = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "\n",
        "clip_transform = transforms.Compose([\n",
        "        transforms.Resize((res, res)),\n",
        "        clip_normalizer\n",
        "])\n",
        "\n",
        "augment_transform = transforms.Compose([\n",
        "        transforms.RandomResizedCrop(res, scale=(1, 1)),\n",
        "        transforms.RandomPerspective(fill=1, p=0.8, distortion_scale=0.5),\n",
        "        clip_normalizer\n",
        "])\n",
        "\n",
        "# MLP Settings\n",
        "mlp = NeuralHighlighter().to(device)\n",
        "optim = torch.optim.Adam(mlp.parameters(), learning_rate)\n",
        "\n",
        "#introducing learning rate decay\n",
        "#with the prompt horse/saddle the loss plateaus\n",
        "#scheduler = StepLR(optim, step_size=300, gamma=0.1)\n",
        "\n",
        "#scheduler = MultiStepLR(optim, milestones=[300, 1800], gamma=0.1)  # Decay a epoch 300 e 1800\n",
        "\n",
        "\n",
        "# list of possible colors\n",
        "rgb_to_color = {(204/255, 1., 0.): \"highlighter\", (180/255, 180/255, 180/255): \"gray\"}\n",
        "color_to_rgb = {\"highlighter\": [204/255, 1., 0.], \"gray\": [180/255, 180/255, 180/255]}\n",
        "full_colors = [[204/255, 1., 0.], [180/255, 180/255, 180/255]]\n",
        "colors = torch.tensor(full_colors).to(device)\n",
        "\n",
        "name = 'dogPC_d_{}_augs_{}'.format(depth, n_augs)\n",
        "\n",
        "# --- Prompt ---\n",
        "# encode prompt with CLIP\n",
        "clip_model, preprocess = get_clip_model(clip_model_name)\n",
        "#prompts = ['A 3D render of a gray horse with highlighted hat',\n",
        "#           'A 3D render of a gray horse with highlighted shoes',\n",
        "#           'A 3D render of a gray horse with highlighted saddle']\n",
        "prompts = ['A three-dimensional picture of a gray dog with highlighted belt']\n",
        "\n",
        "\n",
        "for i, prompt in enumerate(prompts):\n",
        "\n",
        "  output_dir = './output_{}_{}/'.format(name, i)\n",
        "  Path(os.path.join(output_dir, 'renders')).mkdir(parents=True, exist_ok=True)\n",
        "  log_dir = output_dir\n",
        "\n",
        "  #here we compute the text encoding only once\n",
        "  #if we put it inside the loss, we repeat n_iter times the same computation\n",
        "  with torch.no_grad():\n",
        "    text_input = clip.tokenize([prompt]).to(device)\n",
        "    encoded_text = clip_model.encode_text(text_input)\n",
        "    encoded_text = encoded_text / encoded_text.norm(dim=1, keepdim=True)\n",
        "\n",
        "  vertices = copy.deepcopy(mesh.vertices)\n",
        "  #vertices = torch.tensor(np.asarray(rec_mesh.vertices), dtype=torch.float32, device=device) # Convert vertices to a PyTorch tensor\n",
        "  n_views = 5\n",
        "\n",
        "  losses = []\n",
        "\n",
        "\n",
        "  # Optimization loop\n",
        "  for i in tqdm(range(n_iter)):\n",
        "    optim.zero_grad()\n",
        "\n",
        "    # predict highlight probabilities\n",
        "    pred_class = mlp(vertices)\n",
        "\n",
        "    # color and render mesh\n",
        "    sampled_mesh = mesh\n",
        "    color_mesh(pred_class, sampled_mesh, colors)\n",
        "    rendered_images, elev, azim = render.render_views(sampled_mesh, num_views=n_views,\n",
        "                                                            show=False,\n",
        "                                                            center_azim=0,\n",
        "                                                            center_elev=0,\n",
        "                                                            std=1,\n",
        "                                                            return_views=True,\n",
        "                                                            lighting=True,\n",
        "                                                            background=background)\n",
        "\n",
        "    # Calculate CLIP Loss\n",
        "    loss = clip_loss(rendered_images, encoded_text, clip_transform, augment_transform, clip_model)\n",
        "\n",
        "    #loss = clip_loss_custom(encoded_text, rendered_images, clip_model, preprocess)\n",
        "    loss.backward(retain_graph=True)\n",
        "\n",
        "    optim.step()\n",
        "\n",
        "    #LR decay\n",
        "    #scheduler.step()\n",
        "\n",
        "    # update variables + record loss\n",
        "    with torch.no_grad():\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    # report results\n",
        "    if i % 100 == 0:\n",
        "        print(\"Last 100 CLIP score: {}\".format(np.mean(losses[-100:])))\n",
        "        save_renders(log_dir, i, rendered_images)\n",
        "        with open(os.path.join(log_dir, \"training_info.txt\"), \"a\") as f:\n",
        "            f.write(f\"For iteration {i}... Prompt: {prompt}, Last 100 avg CLIP score: {np.mean(losses[-100:])}, CLIP score {losses[-1]}\\n\")\n",
        "\n",
        "\n",
        "  # save results\n",
        "  save_final_results(log_dir, name, mesh, mlp, vertices, colors, render, background)\n",
        "\n",
        "  # save point cloud results\n",
        "  save_point_cloud_results(log_dir, name)\n",
        "\n",
        "\n",
        "\n",
        "  # Save prompts\n",
        "  with open(os.path.join(output_dir, 'prompt.txt'), \"w\") as f:\n",
        "    f.write(prompt)\n",
        "    f.write(\"\\n\")\n",
        "    f.write(\"initial learning rate:\")\n",
        "    f.write(str(learning_rate))\n",
        "    f.write(\"\\n\")\n",
        "    f.write(\"n_iter:\")\n",
        "    f.write(str(n_iter))\n",
        "    f.write(\"\\n\")\n",
        "    f.write(\"n_augs:\")\n",
        "    f.write(str(n_augs))\n",
        "    f.write(\"\\n\")\n",
        "    f.write(\"n_views:\")\n",
        "    f.write(str(n_views))\n",
        "    f.write(\"\\n\")\n",
        "    f.write(\"clip_model:\")\n",
        "    f.write(clip_model_name)\n",
        "    f.write(\"\\n\")\n",
        "    f.write(\"depth:\")\n",
        "    f.write(str(depth))"
      ],
      "metadata": {
        "id": "nLPYsSozt9mC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation"
      ],
      "metadata": {
        "id": "kHJan6C91uqk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test\n"
      ],
      "metadata": {
        "id": "E6pnu1xq1q0H"
      }
    }
  ]
}