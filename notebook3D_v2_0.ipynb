{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","import os\n","drive.mount('/content/drive', force_remount=True)\n","os.chdir('/content/drive/MyDrive/AML')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eU56G8yEJHX3","executionInfo":{"status":"ok","timestamp":1735064313154,"user_tz":-60,"elapsed":5290,"user":{"displayName":"Daniele Benassi","userId":"00283279675511178830"}},"outputId":"8c9d82d2-b337-4912-8497-d458b39fb5fe"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yMBlhrYSSTyu","outputId":"9d1f6ec4-db77-41dc-8f61-e4cdcc45c19f","executionInfo":{"status":"ok","timestamp":1735063831303,"user_tz":-60,"elapsed":25892,"user":{"displayName":"Daniele Benassi","userId":"00283279675511178830"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-rpki8k3e\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-rpki8k3e\n","  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting ftfy (from clip==1.0)\n","  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (24.2)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.67.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.5.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.20.1+cu121)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.26.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (11.0.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n","Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: clip\n","  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369489 sha256=61e2ddbfc7373871046575a36e16a739de3850ca9f6eeed9f4cdd755824a1dc0\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-urfo2euq/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n","Successfully built clip\n","Installing collected packages: ftfy, clip\n","Successfully installed clip-1.0 ftfy-6.3.1\n","Looking in links: https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.5.1_cu121.html\n","Collecting kaolin==0.17.0\n","  Downloading https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.5.1_cu121/kaolin-0.17.0-cp310-cp310-linux_x86_64.whl (5.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ipycanvas (from kaolin==0.17.0)\n","  Downloading ipycanvas-0.13.3-py2.py3-none-any.whl.metadata (6.3 kB)\n","Requirement already satisfied: ipyevents in /usr/local/lib/python3.10/dist-packages (from kaolin==0.17.0) (2.0.2)\n","Requirement already satisfied: jupyter-client<8 in /usr/local/lib/python3.10/dist-packages (from kaolin==0.17.0) (6.1.12)\n","Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from kaolin==0.17.0) (3.1.0)\n","Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from kaolin==0.17.0) (6.3.3)\n","Collecting comm>=0.1.3 (from kaolin==0.17.0)\n","  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n","Collecting usd-core (from kaolin==0.17.0)\n","  Downloading usd_core-24.11-cp310-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.10/dist-packages (from kaolin==0.17.0) (1.26.4)\n","Collecting pybind11 (from kaolin==0.17.0)\n","  Downloading pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n","Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from kaolin==0.17.0) (11.0.0)\n","Requirement already satisfied: tqdm>=4.51.0 in /usr/local/lib/python3.10/dist-packages (from kaolin==0.17.0) (4.67.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from kaolin==0.17.0) (1.13.1)\n","Collecting pygltflib (from kaolin==0.17.0)\n","  Downloading pygltflib-1.16.3.tar.gz (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting warp-lang (from kaolin==0.17.0)\n","  Downloading warp_lang-1.5.0-py3-none-manylinux2014_x86_64.whl.metadata (25 kB)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from kaolin==0.17.0) (7.34.0)\n","Requirement already satisfied: traitlets>=4 in /usr/local/lib/python3.10/dist-packages (from comm>=0.1.3->kaolin==0.17.0) (5.7.1)\n","Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-client<8->kaolin==0.17.0) (5.7.2)\n","Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter-client<8->kaolin==0.17.0) (24.0.1)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client<8->kaolin==0.17.0) (2.8.2)\n","Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.10/dist-packages (from flask->kaolin==0.17.0) (3.1.3)\n","Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from flask->kaolin==0.17.0) (3.1.4)\n","Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.10/dist-packages (from flask->kaolin==0.17.0) (2.2.0)\n","Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from flask->kaolin==0.17.0) (8.1.7)\n","Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.10/dist-packages (from flask->kaolin==0.17.0) (1.9.0)\n","Requirement already satisfied: ipywidgets<9,>=7.6.0 in /usr/local/lib/python3.10/dist-packages (from ipycanvas->kaolin==0.17.0) (7.7.1)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->kaolin==0.17.0) (75.1.0)\n","Collecting jedi>=0.16 (from ipython->kaolin==0.17.0)\n","  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->kaolin==0.17.0) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->kaolin==0.17.0) (0.7.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->kaolin==0.17.0) (3.0.48)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->kaolin==0.17.0) (2.18.0)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->kaolin==0.17.0) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->kaolin==0.17.0) (0.1.7)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->kaolin==0.17.0) (4.9.0)\n","Collecting dataclasses-json>=0.0.25 (from pygltflib->kaolin==0.17.0)\n","  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n","Requirement already satisfied: deprecated in /usr/local/lib/python3.10/dist-packages (from pygltflib->kaolin==0.17.0) (1.2.15)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json>=0.0.25->pygltflib->kaolin==0.17.0)\n","  Downloading marshmallow-3.23.2-py3-none-any.whl.metadata (7.1 kB)\n","Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json>=0.0.25->pygltflib->kaolin==0.17.0)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (5.5.6)\n","Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (0.2.0)\n","Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (3.6.10)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (3.0.13)\n","Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->kaolin==0.17.0) (0.8.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1.2->flask->kaolin==0.17.0) (3.0.2)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.6.0->jupyter-client<8->kaolin==0.17.0) (4.3.6)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->kaolin==0.17.0) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->kaolin==0.17.0) (0.2.13)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.1->jupyter-client<8->kaolin==0.17.0) (1.17.0)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated->pygltflib->kaolin==0.17.0) (1.17.0)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json>=0.0.25->pygltflib->kaolin==0.17.0) (24.2)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json>=0.0.25->pygltflib->kaolin==0.17.0)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json>=0.0.25->pygltflib->kaolin==0.17.0) (4.12.2)\n","Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (6.5.5)\n","Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (23.1.0)\n","Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (5.10.4)\n","Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (7.16.4)\n","Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (1.6.0)\n","Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (1.8.3)\n","Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (0.18.1)\n","Requirement already satisfied: prometheus-client in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (0.21.1)\n","Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (1.1.0)\n","Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (0.2.4)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (4.12.3)\n","Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (6.2.0)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (0.7.1)\n","Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (0.3.0)\n","Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (3.0.2)\n","Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (0.10.1)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (1.5.1)\n","Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (1.4.0)\n","Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (2.21.1)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (4.23.0)\n","Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (21.2.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (0.5.1)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (24.3.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (2024.10.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (0.35.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (0.22.3)\n","Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.10/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (1.24.0)\n","Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (1.17.1)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (2.6)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (2.22)\n","Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (3.7.1)\n","Requirement already satisfied: websocket-client in /usr/local/lib/python3.10/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (1.8.0)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (3.10)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets<9,>=7.6.0->ipycanvas->kaolin==0.17.0) (1.2.2)\n","Downloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n","Downloading ipycanvas-0.13.3-py2.py3-none-any.whl (125 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.8/125.8 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pybind11-2.13.6-py3-none-any.whl (243 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading usd_core-24.11-cp310-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m25.4/25.4 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading warp_lang-1.5.0-py3-none-manylinux2014_x86_64.whl (84.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.6/84.6 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n","Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading marshmallow-3.23.2-py3-none-any.whl (49 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Building wheels for collected packages: pygltflib\n","  Building wheel for pygltflib (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pygltflib: filename=pygltflib-1.16.3-py3-none-any.whl size=27408 sha256=5c7bf8a18aeb73bf1eac368f22bb4eb83e96fe8a2ab95c95db5f5a583805eb28\n","  Stored in directory: /root/.cache/pip/wheels/de/4a/cc/0d166b319ddda5007d0dfa6087346a30c4713b0fdaeaeff304\n","Successfully built pygltflib\n","Installing collected packages: warp-lang, usd-core, pybind11, mypy-extensions, marshmallow, jedi, comm, typing-inspect, dataclasses-json, pygltflib, ipycanvas, kaolin\n","Successfully installed comm-0.2.2 dataclasses-json-0.6.7 ipycanvas-0.13.3 jedi-0.19.2 kaolin-0.17.0 marshmallow-3.23.2 mypy-extensions-1.0.0 pybind11-2.13.6 pygltflib-1.16.3 typing-inspect-0.9.0 usd-core-24.11 warp-lang-1.5.0\n"]}],"source":["!pip install git+https://github.com/openai/CLIP.git\n","!pip install kaolin==0.17.0 -f https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.5.1_cu121.html"]},{"cell_type":"code","source":["#Avaiable clip modules\n","import clip\n","clip.available_models()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6A37o1k6Uugk","outputId":"1f3a5db0-b2a4-4e98-93a1-4ea5dce0e732","executionInfo":{"status":"ok","timestamp":1735063844239,"user_tz":-60,"elapsed":7296,"user":{"displayName":"Daniele Benassi","userId":"00283279675511178830"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['RN50',\n"," 'RN101',\n"," 'RN50x4',\n"," 'RN50x16',\n"," 'RN50x64',\n"," 'ViT-B/32',\n"," 'ViT-B/16',\n"," 'ViT-L/14',\n"," 'ViT-L/14@336px']"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zwjtz3i1STyw"},"outputs":[],"source":["import clip\n","import copy\n","import json\n","import kaolin as kal\n","import kaolin.ops.mesh\n","import numpy as np\n","import os\n","import random\n","import torch\n","import torch.nn as nn\n","import torchvision\n","from torchvision import transforms\n","import torchvision.transforms.functional as F\n","\n","from itertools import permutations, product\n","from Normalization import MeshNormalizer\n","from render import Renderer\n","from mesh import Mesh\n","from pathlib import Path\n","from tqdm import tqdm\n","from torch.autograd import grad\n","from torchvision import transforms\n","from utils import device, color_mesh\n","\n","width = 256\n","depth = 4\n","out_dim = 2\n","input_dim = 3\n","n_augs = 1      #default is 1\n","\n","class NeuralHighlighter(nn.Module):\n","    def __init__(self):\n","        super(NeuralHighlighter, self).__init__()\n","        input_size = 3 #Dimension of the vertex\n","        output_size = 2 #Dimension of the output\n","                        #for the standard highlighter task there are only 2 classes: target region and not target region.\n","                        #we use the element of the output vector corresponding to the probability of belonging to the target\n","                        #region as the highlight probability described in the main paper.\n","        layers = []\n","\n","        #See Appendix B (page 13)\n","        #first linear layer followed by ReLU and LayerNorm\n","        layers.append(nn.Linear(input_dim, width))\n","        layers.append(nn.ReLU())\n","        layers.append(nn.LayerNorm([width]))\n","        #other [depth] linear layers followed by ReLU and LayerNorm\n","        # -> changing the depth hyperparameter results in a deeper/shallower net\n","        # -> total depth = [depth] + 2\n","        for i in range(depth):\n","            layers.append(nn.Linear(width, width))\n","            layers.append(nn.ReLU())\n","            layers.append(nn.LayerNorm([width]))\n","        #last linear layer followed by softmax in order to output probability-like values\n","        layers.append(nn.Linear(width, out_dim))\n","        layers.append(nn.Softmax(dim=1))\n","\n","        self.mlp = nn.ModuleList(layers)\n","        self.model = self.mlp\n","        print(self.mlp)\n","\n","    def forward(self, x):\n","        for layer in self.model:\n","            x = layer(x)\n","        return x\n","\n","def get_clip_model(clipmodel):\n","    model, preprocess = clip.load(clipmodel, device=device)\n","    return model, preprocess\n","\n","# ================== HELPER FUNCTIONS =============================\n","def save_final_results(log_dir, name, mesh, mlp, vertices, colors, render, background):\n","    mlp.eval()\n","    with torch.no_grad():\n","        probs = mlp(vertices)\n","        max_idx = torch.argmax(probs, 1, keepdim=True)\n","        # for renders\n","        one_hot = torch.zeros(probs.shape).to(device)\n","        one_hot = one_hot.scatter_(1, max_idx, 1)\n","        sampled_mesh = mesh\n","\n","        highlight = torch.tensor([204, 255, 0]).to(device)\n","        gray = torch.tensor([180, 180, 180]).to(device)\n","        colors = torch.stack((highlight/255, gray/255)).to(device)\n","        color_mesh(one_hot, sampled_mesh, colors)\n","        rendered_images, _, _ = render.render_views(sampled_mesh, num_views=5,\n","                                                                        show=False,\n","                                                                        center_azim=0,\n","                                                                        center_elev=0,\n","                                                                        std=1,\n","                                                                        return_views=True,\n","                                                                        lighting=True,\n","                                                                        background=background)\n","        # for mesh\n","        final_color = torch.zeros(vertices.shape[0], 3).to(device)\n","        final_color = torch.where(max_idx==0, highlight, gray)\n","        mesh.export(os.path.join(log_dir, f\"{name}.ply\"), extension=\"ply\", color=final_color)\n","        save_renders(log_dir, 0, rendered_images, name='final_render.jpg')\n","\n","def clip_loss(rendered_images, encoded_text, clip_transform, augment_transform, clip_model):\n","    if n_augs == 0:\n","        clip_image = clip_transform(rendered_images)\n","        encoded_renders = clip_model.encode_image(clip_image)\n","        encoded_renders = encoded_renders / encoded_renders.norm(dim=1, keepdim=True)\n","        if encoded_text.shape[0] > 1:\n","            loss = torch.cosine_similarity(torch.mean(encoded_renders, dim=0),\n","                                                torch.mean(encoded_text, dim=0), dim=0)\n","        else:\n","            loss = torch.cosine_similarity(torch.mean(encoded_renders, dim=0, keepdim=True),\n","                                                encoded_text)\n","\n","    elif n_augs > 0:\n","        loss = 0.0\n","        for _ in range(n_augs):\n","            augmented_image = augment_transform(rendered_images)\n","            encoded_renders = clip_model.encode_image(augmented_image)\n","            if encoded_text.shape[0] > 1:\n","                loss -= torch.cosine_similarity(torch.mean(encoded_renders, dim=0),\n","                                                    torch.mean(encoded_text, dim=0), dim=0)\n","            else:\n","                loss -= torch.cosine_similarity(torch.mean(encoded_renders, dim=0, keepdim=True),\n","                                                    encoded_text)\n","    return loss\n","\n","\"\"\"\n","def clip_loss(prompt, rendered_images, clip_model ):\n","    # compute cosine similarity\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    model, preprocess = get_clip_model(clip_model)\n","    text_input = clip.tokenize(prompt).to(device)\n","    text_embedding = model.encode_text(text_input)\n","    # Make sure the text_embedding is 1D (flatten) for the dot product of the clip loss\n","    text_embedding = text_embedding.flatten()\n","    images_embeddings = []\n","    # Compute aggregate image rappresentation\n","    for image in rendered_images:\n","        with torch.no_grad():\n","            image = transforms.ToPILImage()(image)\n","            image_input = preprocess(image).unsqueeze(0).to(device)\n","            images_embeddings.append(model.encode_image(image_input))\n","    images_aggregate = torch.stack(images_embeddings).mean(dim=0)\n","    # Flatten the aggregate image embeddings\n","    images_aggregate = images_aggregate.flatten()\n","\n","    cosine_similarity = torch.dot(images_aggregate, text_embedding) / (torch.norm(images_aggregate) * torch.norm(text_embedding))\n","    return 1 - cosine_similarity\n","\n","\"\"\"\n","\n","def save_renders(dir, i, rendered_images, name=None):\n","    if name is not None:\n","        torchvision.utils.save_image(rendered_images, os.path.join(dir, name))\n","    else:\n","        torchvision.utils.save_image(rendered_images, os.path.join(dir, 'renders/iter_{}.jpg'.format(i)))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NW_UP8wESTyx","outputId":"7dc3555c-c4bc-41d4-9249-8a3e6bfa9c0a","executionInfo":{"status":"ok","timestamp":1735061677636,"user_tz":-60,"elapsed":737730,"user":{"displayName":"Daniele Benassi","userId":"00283279675511178830"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["ModuleList(\n","  (0): Linear(in_features=3, out_features=256, bias=True)\n","  (1): ReLU()\n","  (2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","  (3): Linear(in_features=256, out_features=256, bias=True)\n","  (4): ReLU()\n","  (5): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","  (6): Linear(in_features=256, out_features=256, bias=True)\n","  (7): ReLU()\n","  (8): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","  (9): Linear(in_features=256, out_features=256, bias=True)\n","  (10): ReLU()\n","  (11): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","  (12): Linear(in_features=256, out_features=256, bias=True)\n","  (13): ReLU()\n","  (14): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","  (15): Linear(in_features=256, out_features=2, bias=True)\n","  (16): Softmax(dim=1)\n",")\n"]},{"output_type":"stream","name":"stderr","text":["  0%|          | 1/2500 [00:00<14:10,  2.94it/s]"]},{"output_type":"stream","name":"stdout","text":["Last 100 CLIP score: -0.76025390625\n"]},{"output_type":"stream","name":"stderr","text":["  4%|▍         | 101/2500 [00:29<11:54,  3.36it/s]"]},{"output_type":"stream","name":"stdout","text":["Last 100 CLIP score: -0.9239453125\n"]},{"output_type":"stream","name":"stderr","text":["  8%|▊         | 201/2500 [00:58<11:09,  3.44it/s]"]},{"output_type":"stream","name":"stdout","text":["Last 100 CLIP score: -0.93390625\n"]},{"output_type":"stream","name":"stderr","text":[" 12%|█▏        | 301/2500 [01:27<10:54,  3.36it/s]"]},{"output_type":"stream","name":"stdout","text":["Last 100 CLIP score: -0.937705078125\n"]},{"output_type":"stream","name":"stderr","text":[" 16%|█▌        | 401/2500 [01:55<10:12,  3.43it/s]"]},{"output_type":"stream","name":"stdout","text":["Last 100 CLIP score: -0.9158251953125\n"]},{"output_type":"stream","name":"stderr","text":[" 20%|██        | 501/2500 [02:24<09:51,  3.38it/s]"]},{"output_type":"stream","name":"stdout","text":["Last 100 CLIP score: -0.9080517578125\n"]},{"output_type":"stream","name":"stderr","text":[" 24%|██▍       | 601/2500 [02:53<09:17,  3.40it/s]"]},{"output_type":"stream","name":"stdout","text":["Last 100 CLIP score: -0.912490234375\n"]},{"output_type":"stream","name":"stderr","text":[" 28%|██▊       | 701/2500 [03:22<08:50,  3.39it/s]"]},{"output_type":"stream","name":"stdout","text":["Last 100 CLIP score: -0.9174072265625\n"]},{"output_type":"stream","name":"stderr","text":[" 32%|███▏      | 801/2500 [03:51<08:24,  3.37it/s]"]},{"output_type":"stream","name":"stdout","text":["Last 100 CLIP score: -0.9135595703125\n"]},{"output_type":"stream","name":"stderr","text":[" 36%|███▌      | 901/2500 [04:20<07:53,  3.38it/s]"]},{"output_type":"stream","name":"stdout","text":["Last 100 CLIP score: -0.91802734375\n"]},{"output_type":"stream","name":"stderr","text":[" 40%|████      | 1001/2500 [04:49<07:18,  3.42it/s]"]},{"output_type":"stream","name":"stdout","text":["Last 100 CLIP score: -0.91509765625\n"]},{"output_type":"stream","name":"stderr","text":[" 44%|████▍     | 1101/2500 [05:18<06:55,  3.37it/s]"]},{"output_type":"stream","name":"stdout","text":["Last 100 CLIP score: -0.9176220703125\n"]},{"output_type":"stream","name":"stderr","text":[" 48%|████▊     | 1201/2500 [05:47<06:26,  3.36it/s]"]},{"output_type":"stream","name":"stdout","text":["Last 100 CLIP score: -0.9140087890625\n"]},{"output_type":"stream","name":"stderr","text":[" 52%|█████▏    | 1301/2500 [06:15<05:49,  3.43it/s]"]},{"output_type":"stream","name":"stdout","text":["Last 100 CLIP score: -0.92123046875\n"]},{"output_type":"stream","name":"stderr","text":[" 56%|█████▌    | 1401/2500 [06:44<05:24,  3.39it/s]"]},{"output_type":"stream","name":"stdout","text":["Last 100 CLIP score: -0.917197265625\n"]},{"output_type":"stream","name":"stderr","text":[" 60%|██████    | 1501/2500 [07:13<04:56,  3.37it/s]"]},{"output_type":"stream","name":"stdout","text":["Last 100 CLIP score: -0.9155126953125\n"]},{"output_type":"stream","name":"stderr","text":[" 64%|██████▍   | 1601/2500 [07:42<04:27,  3.37it/s]"]},{"output_type":"stream","name":"stdout","text":["Last 100 CLIP score: -0.915986328125\n"]},{"output_type":"stream","name":"stderr","text":[" 68%|██████▊   | 1701/2500 [08:11<04:04,  3.27it/s]"]},{"output_type":"stream","name":"stdout","text":["Last 100 CLIP score: -0.919287109375\n"]},{"output_type":"stream","name":"stderr","text":[" 72%|███████▏  | 1801/2500 [08:40<03:25,  3.40it/s]"]},{"output_type":"stream","name":"stdout","text":["Last 100 CLIP score: -0.919912109375\n"]},{"output_type":"stream","name":"stderr","text":[" 76%|███████▌  | 1901/2500 [09:09<02:56,  3.40it/s]"]},{"output_type":"stream","name":"stdout","text":["Last 100 CLIP score: -0.920869140625\n"]},{"output_type":"stream","name":"stderr","text":[" 80%|████████  | 2001/2500 [09:38<02:27,  3.38it/s]"]},{"output_type":"stream","name":"stdout","text":["Last 100 CLIP score: -0.9096337890625\n"]},{"output_type":"stream","name":"stderr","text":[" 84%|████████▍ | 2101/2500 [10:06<01:56,  3.41it/s]"]},{"output_type":"stream","name":"stdout","text":["Last 100 CLIP score: -0.91923828125\n"]},{"output_type":"stream","name":"stderr","text":[" 88%|████████▊ | 2201/2500 [10:35<01:28,  3.38it/s]"]},{"output_type":"stream","name":"stdout","text":["Last 100 CLIP score: -0.9189599609375\n"]},{"output_type":"stream","name":"stderr","text":[" 92%|█████████▏| 2301/2500 [11:04<00:58,  3.39it/s]"]},{"output_type":"stream","name":"stdout","text":["Last 100 CLIP score: -0.9219775390625\n"]},{"output_type":"stream","name":"stderr","text":[" 96%|█████████▌| 2401/2500 [11:33<00:29,  3.37it/s]"]},{"output_type":"stream","name":"stdout","text":["Last 100 CLIP score: -0.9227490234375\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 2500/2500 [12:02<00:00,  3.46it/s]\n"]}],"source":["# Constrain most sources of randomness\n","# (some torch backwards functions within CLIP are non-determinstic)\n","seed = 0\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True\n","\n","render_res = 224\n","learning_rate = 0.0001\n","n_iter = 2500\n","res = 224\n","obj_path = 'data/horse.obj'\n","output_dir = './output/'\n","clip_model_name = 'ViT-L/14'\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","Path(os.path.join(output_dir, 'renders')).mkdir(parents=True, exist_ok=True)\n","\n","objbase, extension = os.path.splitext(os.path.basename(obj_path))\n","\n","render = Renderer(dim=(render_res, render_res))\n","mesh = Mesh(obj_path)\n","MeshNormalizer(mesh)()\n","\n","# Initialize variables\n","background = torch.tensor((1., 1., 1.)).to(device)\n","\n","log_dir = output_dir\n","# CLIP and Augmentation Transforms\n","clip_normalizer = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n","\n","clip_transform = transforms.Compose([\n","        transforms.Resize((res, res)),\n","        clip_normalizer\n","])\n","\n","augment_transform = transforms.Compose([\n","        transforms.RandomResizedCrop(res, scale=(1, 1)),\n","        transforms.RandomPerspective(fill=1, p=0.8, distortion_scale=0.5),\n","        clip_normalizer\n","])\n","\n","# MLP Settings\n","mlp = NeuralHighlighter().to(device)\n","optim = torch.optim.Adam(mlp.parameters(), learning_rate)\n","\n","# list of possible colors\n","rgb_to_color = {(204/255, 1., 0.): \"highlighter\", (180/255, 180/255, 180/255): \"gray\"}\n","color_to_rgb = {\"highlighter\": [204/255, 1., 0.], \"gray\": [180/255, 180/255, 180/255]}\n","full_colors = [[204/255, 1., 0.], [180/255, 180/255, 180/255]]\n","colors = torch.tensor(full_colors).to(device)\n","\n","name = \"horse\"\n","\n","# --- Prompt ---\n","# encode prompt with CLIP\n","clip_model, preprocess = get_clip_model(clip_model_name)\n","prompt = \"A 3D render of a gray horse with highlighted shoes\"\n","\n","#here we compute the text encoding only once\n","#if we put it inside the loss, we repeat n_iter times the same computation\n","with torch.no_grad():\n","    text_input = clip.tokenize([prompt]).to(device)\n","    encoded_text = clip_model.encode_text(text_input)\n","    encoded_text = encoded_text / encoded_text.norm(dim=1, keepdim=True)\n","\n","vertices = copy.deepcopy(mesh.vertices)\n","n_views = 5\n","\n","losses = []\n","\n","# Optimization loop\n","for i in tqdm(range(n_iter)):\n","    optim.zero_grad()\n","\n","    # predict highlight probabilities\n","    pred_class = mlp(vertices)\n","\n","    # color and render mesh\n","    sampled_mesh = mesh\n","    color_mesh(pred_class, sampled_mesh, colors)\n","    rendered_images, elev, azim = render.render_views(sampled_mesh, num_views=n_views,\n","                                                            show=False,\n","                                                            center_azim=0,\n","                                                            center_elev=0,\n","                                                            std=1,\n","                                                            return_views=True,\n","                                                            lighting=True,\n","                                                            background=background)\n","\n","    # Calculate CLIP Loss\n","    loss = clip_loss(rendered_images, encoded_text, clip_transform, augment_transform, clip_model)\n","    loss.backward(retain_graph=True)\n","\n","    optim.step()\n","\n","    # update variables + record loss\n","    with torch.no_grad():\n","        losses.append(loss.item())\n","\n","    # report results\n","    if i % 100 == 0:\n","        print(\"Last 100 CLIP score: {}\".format(np.mean(losses[-100:])))\n","        save_renders(log_dir, i, rendered_images)\n","        with open(os.path.join(log_dir, \"training_info.txt\"), \"a\") as f:\n","            f.write(f\"For iteration {i}... Prompt: {prompt}, Last 100 avg CLIP score: {np.mean(losses[-100:])}, CLIP score {losses[-1]}\\n\")\n","\n","\n","# save results\n","save_final_results(log_dir, name, mesh, mlp, vertices, colors, render, background)\n","\n","# Save prompts\n","with open(os.path.join(output_dir, \"prompt.txt\"), \"w\") as f:\n","    f.write(prompt)\n","    f.write(\"\\n\")\n","    f.write(\"learning rate:\")\n","    f.write(str(learning_rate))\n","    f.write(\"\\n\")\n","    f.write(\"n_iter:\")\n","    f.write(str(n_iter))\n","    f.write(\"\\n\")\n","    f.write(\"n_augs:\")\n","    f.write(str(n_augs))\n","    f.write(\"\\n\")\n","    f.write(\"clip_model:\")\n","    f.write(clip_model_name)\n","    f.write(\"\\n\")\n","    f.write(\"depth:\")\n","    f.write(str(depth))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"colab":{"provenance":[],"gpuType":"T4","machine_shape":"hm"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}