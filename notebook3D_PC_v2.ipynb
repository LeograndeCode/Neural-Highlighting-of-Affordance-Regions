{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"BzacayYuedGi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735931590272,"user_tz":-60,"elapsed":2056,"user":{"displayName":"Daniele Benassi","userId":"00283279675511178830"}},"outputId":"ba90cf00-a46d-4fe5-b38c-55c6fe08716a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/Neural-Highlighting-of-Affordance-Regions\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive') #replace with drive.mount('/content/drive/', force_remount=True) if the drive has changed since last mount in order to force the remount\n","%cd /content/drive/MyDrive/Neural-Highlighting-of-Affordance-Regions/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yMBlhrYSSTyu"},"outputs":[],"source":["!apt-get update\n","!apt-get install -y xvfb ffmpeg libsm6 libxext6\n","!pip install git+https://github.com/openai/CLIP.git\n","!pip install kaolin==0.17.0 -f https://nvidia-kaolin.s3.us-east-2.amazonaws.com/torch-2.5.1_cu121.html\n","!pip install open3d pyvirtualdisplay"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"6A37o1k6Uugk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1735931612269,"user_tz":-60,"elapsed":3193,"user":{"displayName":"Daniele Benassi","userId":"00283279675511178830"}},"outputId":"0533a204-e85f-4abe-c47a-4a04616993d8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['RN50',\n"," 'RN101',\n"," 'RN50x4',\n"," 'RN50x16',\n"," 'RN50x64',\n"," 'ViT-B/32',\n"," 'ViT-B/16',\n"," 'ViT-L/14',\n"," 'ViT-L/14@336px']"]},"metadata":{},"execution_count":3}],"source":["#Available clip models\n","import clip\n","clip.available_models()\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":3441,"status":"ok","timestamp":1735931628448,"user":{"displayName":"Daniele Benassi","userId":"00283279675511178830"},"user_tz":-60},"id":"zwjtz3i1STyw","colab":{"base_uri":"https://localhost:8080/","height":139},"outputId":"24c46aaa-897a-4a08-a76f-e3ad8caac5a3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Warp 1.5.1 initialized:\n","   CUDA Toolkit 12.6, Driver 12.2\n","   Devices:\n","     \"cpu\"      : \"x86_64\"\n","     \"cuda:0\"   : \"Tesla T4\" (15 GiB, sm_75, mempool enabled)\n","   Kernel cache:\n","     /root/.cache/warp/1.5.1\n"]}],"source":["import clip\n","import copy\n","import json\n","import kaolin as kal\n","import kaolin.ops.mesh\n","import numpy as np\n","import os\n","import random\n","import torch\n","import torch.nn as nn\n","import torchvision\n","from torchvision import transforms\n","import torchvision.transforms.functional as F\n","\n","from itertools import permutations, product\n","from Normalization import MeshNormalizer\n","from render import Renderer\n","from mesh import Mesh\n","from pathlib import Path\n","from tqdm import tqdm\n","from torch.autograd import grad\n","from torchvision import transforms\n","from utils import device, color_mesh\n","import open3d as o3d\n","from pyvirtualdisplay import Display\n","\n","width = 256\n","depth = 4       #default is 4\n","out_dim = 2\n","input_dim = 3\n","n_augs = 1      #default is 1\n","\n","class NeuralHighlighter(nn.Module):\n","    def __init__(self):\n","        super(NeuralHighlighter, self).__init__()\n","        input_size = 3 #Dimension of the vertex\n","        output_size = 2 #Dimension of the output\n","                        #for the standard highlighter task there are only 2 classes: target region and not target region.\n","                        #we use the element of the output vector corresponding to the probability of belonging to the target\n","                        #region as the highlight probability described in the main paper.\n","        layers = []\n","\n","        #See Appendix B (page 13)\n","        #first linear layer followed by ReLU and LayerNorm\n","        layers.append(nn.Linear(input_dim, width))\n","        layers.append(nn.ReLU())\n","        layers.append(nn.LayerNorm([width]))\n","        #other [depth] linear layers followed by ReLU and LayerNorm\n","        # -> changing the depth hyperparameter results in a deeper/shallower net\n","        # -> total depth (in terms of modules[Linear+ReLU+LayerNorm]) = [depth] + 1\n","        for i in range(depth):\n","            layers.append(nn.Linear(width, width))\n","            layers.append(nn.ReLU())\n","            layers.append(nn.LayerNorm([width]))\n","        #last linear layer followed by softmax in order to output probability-like values\n","        layers.append(nn.Linear(width, out_dim))\n","        layers.append(nn.Softmax(dim=1))\n","\n","        self.mlp = nn.ModuleList(layers)\n","        self.model = self.mlp\n","        print(self.mlp)\n","\n","    def forward(self, x):\n","        for layer in self.model:\n","            x = layer(x)\n","        return x\n","\n","def get_clip_model(clipmodel):\n","    model, preprocess = clip.load(clipmodel, device=device)\n","    return model, preprocess\n","\n","# ================== HELPER FUNCTIONS =============================\n","def save_final_results(log_dir, name, mesh, mlp, vertices, colors, render, background):\n","    mlp.eval()\n","    with torch.no_grad():\n","        probs = mlp(vertices)\n","        max_idx = torch.argmax(probs, 1, keepdim=True)\n","        # for renders\n","        one_hot = torch.zeros(probs.shape).to(device)\n","        one_hot = one_hot.scatter_(1, max_idx, 1)\n","        sampled_mesh = mesh\n","\n","        highlight = torch.tensor([204, 255, 0]).to(device)\n","        gray = torch.tensor([180, 180, 180]).to(device)\n","        colors = torch.stack((highlight/255, gray/255)).to(device)\n","        color_mesh(one_hot, sampled_mesh, colors)\n","        rendered_images, _, _ = render.render_views(sampled_mesh, num_views=5,\n","                                                                        show=False,\n","                                                                        center_azim=0,\n","                                                                        center_elev=0,\n","                                                                        std=1,\n","                                                                        return_views=True,\n","                                                                        lighting=True,\n","                                                                        background=background)\n","        # for mesh\n","        final_color = torch.zeros(vertices.shape[0], 3).to(device)\n","        final_color = torch.where(max_idx==0, highlight, gray)\n","        mesh.export(os.path.join(log_dir, f\"{name}.ply\"), extension=\"ply\", color=final_color)\n","        save_renders(log_dir, 0, rendered_images, name='final_render.jpg')\n","\n","#TODO: fix the generation of the point cloud subsequently\n","#      now the point cloud generation is possible only by executing the PC_rendering.ipynb\n","\n","def save_point_cloud_results(log_dir, name):\n","        #now i load the highlighted mesh and transpose it back to the point cloud\n","        display = Display(visible=0, size=(1400, 900))\n","        display.start()\n","        mesh_o3d = o3d.io.read_triangle_mesh(os.path.join(log_dir, f\"{name}.ply\"))\n","\n","        if not mesh_o3d.has_vertex_normals():\n","          mesh_o3d.compute_vertex_normals()\n","\n","        point_cloud = mesh_o3d.sample_points_poisson_disk(number_of_points=10000)\n","\n","        width_final_render, height_final_render = 1400, 900\n","        render_final_pc = o3d.visualization.rendering.OffscreenRenderer(width_final_render, height_final_render)\n","        material = o3d.visualization.rendering.MaterialRecord()\n","        material.shader = \"defaultUnlit\"\n","        render_final_pc.scene.add_geometry(\"point_cloud\", point_cloud, material)\n","\n","        zoom_out_factor = 0.5\n","        bounding_box = point_cloud.get_axis_aligned_bounding_box()\n","        center = bounding_box.get_center()\n","        extent = bounding_box.get_extent()\n","        render_final_pc.scene.camera.look_at(center, center + [0, 0, 1], [0, 1, 0])\n","        render_final_pc.scene.camera.set_projection(60 / zoom_out_factor, width_final_render / height_final_render, 0.1, 100.0,\n","                                      o3d.visualization.rendering.Camera.FovType.Horizontal)\n","\n","        pc_img = render_final_pc.render_to_image()\n","        output_file = os.path.join(log_dir, f\"{name}_final_render.jpg\")\n","        o3d.io.write_image(output_file, pc_img)\n","        display.stop()\n","\n","\n","def clip_loss(rendered_images, encoded_text, clip_transform, augment_transform, clip_model):\n","    if n_augs == 0:\n","        clip_image = clip_transform(rendered_images)\n","        encoded_renders = clip_model.encode_image(clip_image)\n","        encoded_renders = encoded_renders / encoded_renders.norm(dim=1, keepdim=True)\n","        if encoded_text.shape[0] > 1:\n","            loss = torch.cosine_similarity(torch.mean(encoded_renders, dim=0),\n","                                                torch.mean(encoded_text, dim=0), dim=0)\n","        else:\n","            loss = torch.cosine_similarity(torch.mean(encoded_renders, dim=0, keepdim=True),\n","                                                encoded_text)\n","\n","    elif n_augs > 0:\n","        loss = 0.0 #original 0.0\n","        for _ in range(n_augs):\n","            augmented_image = augment_transform(rendered_images)\n","            encoded_renders = clip_model.encode_image(augmented_image)\n","            if encoded_text.shape[0] > 1:\n","                loss -= torch.cosine_similarity(torch.mean(encoded_renders, dim=0),\n","                                                    torch.mean(encoded_text, dim=0), dim=0)\n","            else:\n","                loss -= torch.cosine_similarity(torch.mean(encoded_renders, dim=0, keepdim=True),\n","                                                    encoded_text)\n","    return loss\n","\n","def save_renders(dir, i, rendered_images, name=None):\n","    if name is not None:\n","        torchvision.utils.save_image(rendered_images, os.path.join(dir, name))\n","    else:\n","        torchvision.utils.save_image(rendered_images, os.path.join(dir, 'renders/iter_{}.jpg'.format(i)))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NW_UP8wESTyx"},"outputs":[],"source":["from torch.optim.lr_scheduler import MultiStepLR\n","# Constrain most sources of randomness\n","# (some torch backwards functions within CLIP are non-determinstic)\n","seed = 0\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True\n","\n","render_res = 224\n","learning_rate = 0.0001\n","n_iter = 2500\n","res = 224\n","obj_path = 'data/dog.obj'\n","#output_dir = './output/'\n","clip_model_name = 'ViT-B/32'\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","#Path(os.path.join(output_dir, 'renders')).mkdir(parents=True, exist_ok=True)\n","\n","objbase, extension = os.path.splitext(os.path.basename(obj_path))\n","\n","render = Renderer(dim=(render_res, render_res))\n","\n","#---------- WARNING---------\n","#---------- now i read the mesh from the obj of the reconstructed mesh\n","#mesh = Mesh(obj_path)\n","#MeshNormalizer(mesh)()\n","#---------- WARNING---------\n","\n","\n","#------------ MESH TO POINT CLOUD INIT---------------\n","from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(1400, 900))\n","display.start()\n","\n","#here we retrieve first the point cloud from the mesh (only for test purpose)\n","mesh_o3d = o3d.io.read_triangle_mesh(obj_path)\n","mesh_o3d.compute_vertex_normals()\n","pcd = mesh_o3d.sample_points_poisson_disk(10000)\n","\n","width_render, height_render = 1400, 900\n","render_pc = o3d.visualization.rendering.OffscreenRenderer(width_render, height_render)\n","material = o3d.visualization.rendering.MaterialRecord()\n","material.shader = \"defaultUnlit\"\n","render_pc.scene.add_geometry(\"point_cloud\", pcd, material)\n","\n","zoom_out_factor = 0.5\n","bounding_box = pcd.get_axis_aligned_bounding_box()\n","center = bounding_box.get_center()\n","extent = bounding_box.get_extent()\n","render_pc.scene.camera.look_at(center, center + [0, 0, 1], [0, 1, 0])\n","render_pc.scene.camera.set_projection(60 / zoom_out_factor, width_render / height_render, 0.1, 100.0,\n","                                      o3d.visualization.rendering.Camera.FovType.Horizontal)\n","\n","img = render_pc.render_to_image()\n","output_file = \"point_cloud_render.jpg\"\n","o3d.io.write_image(output_file, img)\n","#------------ MESH TO POINT CLOUD END---------------\n","#------------ POINT CLOUD TO MESH INIT---------------\n","radii = [0.005, 0.01, 0.02, 0.04]\n","rec_mesh = o3d.geometry.TriangleMesh.create_from_point_cloud_ball_pivoting(pcd, o3d.utility.DoubleVector(radii))\n","\n","#rec_mesh.vertices = o3d.utility.Vector3dVector(np.asarray(rec_mesh.vertices))\n","#rec_mesh.triangles = o3d.utility.Vector3iVector(np.asarray(rec_mesh.triangles))\n","\n","# Optionally, check if the reconstructed mesh is valid\n","if rec_mesh.is_empty():\n","    print(\"Mesh reconstruction failed.\")\n","else:\n","    print(\"Mesh reconstruction successful!\")\n","\n","# Render the reconstructed mesh\n","render_mesh = o3d.visualization.rendering.OffscreenRenderer(width_render, height_render)\n","material_mesh = o3d.visualization.rendering.MaterialRecord()\n","material_mesh.shader = \"defaultUnlit\"\n","render_mesh.scene.add_geometry(\"reconstructed_mesh\", rec_mesh, material_mesh)\n","\n","# Set zoom-out factor and camera for reconstructed mesh\n","render_mesh.scene.camera.look_at(center, center + [0, 0, 1], [0, 1, 0])\n","render_mesh.scene.camera.set_projection(60 / zoom_out_factor, width_render / height_render, 0.1, 100.0,\n","                                        o3d.visualization.rendering.Camera.FovType.Horizontal)\n","\n","# Render and save the reconstructed mesh image\n","img_mesh = render_mesh.render_to_image()\n","output_file_mesh = \"reconstructed_mesh_render.jpg\"\n","o3d.io.write_image(output_file_mesh, img_mesh)\n","\n","#Export the reconstructed mesh to an obj file (allows to reuse helper functions)\n","output_mesh_file = \"reconstructed_mesh.obj\"\n","o3d.io.write_triangle_mesh(output_mesh_file, rec_mesh)\n","\n","#Import the mesh from the exported obj file\n","mesh = Mesh(output_mesh_file)\n","MeshNormalizer(mesh)()\n","\n","display.stop()\n","#------------ POINT CLOUD TO MESH END---------------\n","\n","#then we approximate a mesh so we can still use the previously defined helper functions\n","#also the loss minimization should converge better\n","\n","\n","# Initialize variables\n","background = torch.tensor((1., 1., 1.)).to(device)\n","\n","#log_dir = output_dir\n","\n","# CLIP and Augmentation Transforms\n","clip_normalizer = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n","\n","clip_transform = transforms.Compose([\n","        transforms.Resize((res, res)),\n","        clip_normalizer\n","])\n","\n","augment_transform = transforms.Compose([\n","        transforms.RandomResizedCrop(res, scale=(1, 1)),\n","        transforms.RandomPerspective(fill=1, p=0.8, distortion_scale=0.5),\n","        clip_normalizer\n","])\n","\n","# MLP Settings\n","mlp = NeuralHighlighter().to(device)\n","optim = torch.optim.Adam(mlp.parameters(), learning_rate)\n","\n","#introducing learning rate decay\n","#with the prompt horse/saddle the loss plateaus\n","#scheduler = StepLR(optim, step_size=300, gamma=0.1)\n","\n","#scheduler = MultiStepLR(optim, milestones=[300, 1800], gamma=0.1)  # Decay a epoch 300 e 1800\n","\n","\n","# list of possible colors\n","rgb_to_color = {(204/255, 1., 0.): \"highlighter\", (180/255, 180/255, 180/255): \"gray\"}\n","color_to_rgb = {\"highlighter\": [204/255, 1., 0.], \"gray\": [180/255, 180/255, 180/255]}\n","full_colors = [[204/255, 1., 0.], [180/255, 180/255, 180/255]]\n","colors = torch.tensor(full_colors).to(device)\n","\n","name = 'dogPC_d_{}_augs_{}'.format(depth, n_augs)\n","\n","# --- Prompt ---\n","# encode prompt with CLIP\n","clip_model, preprocess = get_clip_model(clip_model_name)\n","#prompts = ['A 3D render of a gray horse with highlighted hat',\n","#           'A 3D render of a gray horse with highlighted shoes',\n","#           'A 3D render of a gray horse with highlighted saddle']\n","prompts = ['A three-dimensional picture of a gray dog with highlighted belt']\n","\n","\n","for i, prompt in enumerate(prompts):\n","\n","  output_dir = './output_{}_{}/'.format(name, i)\n","  Path(os.path.join(output_dir, 'renders')).mkdir(parents=True, exist_ok=True)\n","  log_dir = output_dir\n","\n","  #here we compute the text encoding only once\n","  #if we put it inside the loss, we repeat n_iter times the same computation\n","  with torch.no_grad():\n","    text_input = clip.tokenize([prompt]).to(device)\n","    encoded_text = clip_model.encode_text(text_input)\n","    encoded_text = encoded_text / encoded_text.norm(dim=1, keepdim=True)\n","\n","  vertices = copy.deepcopy(mesh.vertices)\n","  #vertices = torch.tensor(np.asarray(rec_mesh.vertices), dtype=torch.float32, device=device) # Convert vertices to a PyTorch tensor\n","  n_views = 5\n","\n","  losses = []\n","\n","\n","  # Optimization loop\n","  for i in tqdm(range(n_iter)):\n","    optim.zero_grad()\n","\n","    # predict highlight probabilities\n","    pred_class = mlp(vertices)\n","\n","    # color and render mesh\n","    sampled_mesh = mesh\n","    color_mesh(pred_class, sampled_mesh, colors)\n","    rendered_images, elev, azim = render.render_views(sampled_mesh, num_views=n_views,\n","                                                            show=False,\n","                                                            center_azim=0,\n","                                                            center_elev=0,\n","                                                            std=1,\n","                                                            return_views=True,\n","                                                            lighting=True,\n","                                                            background=background)\n","\n","    # Calculate CLIP Loss\n","    loss = clip_loss(rendered_images, encoded_text, clip_transform, augment_transform, clip_model)\n","\n","    #loss = clip_loss_custom(encoded_text, rendered_images, clip_model, preprocess)\n","    loss.backward(retain_graph=True)\n","\n","    optim.step()\n","\n","    #LR decay\n","    #scheduler.step()\n","\n","    # update variables + record loss\n","    with torch.no_grad():\n","        losses.append(loss.item())\n","\n","    # report results\n","    if i % 100 == 0:\n","        print(\"Last 100 CLIP score: {}\".format(np.mean(losses[-100:])))\n","        save_renders(log_dir, i, rendered_images)\n","        with open(os.path.join(log_dir, \"training_info.txt\"), \"a\") as f:\n","            f.write(f\"For iteration {i}... Prompt: {prompt}, Last 100 avg CLIP score: {np.mean(losses[-100:])}, CLIP score {losses[-1]}\\n\")\n","\n","\n","  # save results\n","  save_final_results(log_dir, name, mesh, mlp, vertices, colors, render, background)\n","\n","  # save point cloud results\n","  save_point_cloud_results(log_dir, name)\n","\n","\n","\n","  # Save prompts\n","  with open(os.path.join(output_dir, 'prompt.txt'), \"w\") as f:\n","    f.write(prompt)\n","    f.write(\"\\n\")\n","    f.write(\"initial learning rate:\")\n","    f.write(str(learning_rate))\n","    f.write(\"\\n\")\n","    f.write(\"n_iter:\")\n","    f.write(str(n_iter))\n","    f.write(\"\\n\")\n","    f.write(\"n_augs:\")\n","    f.write(str(n_augs))\n","    f.write(\"\\n\")\n","    f.write(\"n_views:\")\n","    f.write(str(n_views))\n","    f.write(\"\\n\")\n","    f.write(\"clip_model:\")\n","    f.write(clip_model_name)\n","    f.write(\"\\n\")\n","    f.write(\"depth:\")\n","    f.write(str(depth))\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}